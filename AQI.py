# -*- coding: utf-8 -*-
"""Assessment 4(18265).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qwv_Sr9DcSrftszt8eCSE49YkadE8aRy
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams
import warnings
from sklearn.preprocessing import MinMaxScaler,RobustScaler
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn import metrics
import keras
from keras import regularizers
from keras.models import Sequential
from keras.layers import Dense,Dropout

def Plots(output,true,observed):
  plt.figure(figsize=(10, 5))
  
  plt.subplot(121)
  plt.title('PM 2.5 Distribution')
  sns.distplot(output)

  plt.subplot(122)
  plt.title('PM 2.5 Distribution for predicted values')
  sns.distplot(true-observed)
  plt.show()

  plt.title('Scatter plot for PM 2.5 real vs predicted')
  plt.scatter(true,observed)
  plt.show()

  plt.figure(figsize=(15,10))
  plt.plot(observed)
  plt.plot(true)
  plt.title('Prediction vs Real PM2.5 values')
  plt.ylabel('PM 2.5')
  plt.xlabel('Days')
  plt.legend(['Prediction', 'Real'], loc='upper left')
  plt.show()

def Analysis(model,x,y,T,O):
  A=[]
  A.append(metrics.mean_absolute_error(T,O))
  A.append(metrics.mean_squared_error(T,O))
  A.append(np.sqrt(metrics.mean_squared_error(T,O)))
  A.append(model.score(x,y))
  return A

def Regression(model,x,y,x_t,y_t):
  model.fit(x,y)
  prediction=model.predict(x_t)
  print(model.fit(x,y))
  Plots(y,y_t,prediction)
  return Analysis(model,x,y,y_t,prediction)

def Information(df):
  print(df.info())
  print(df.describe().T)

def Visual(df):
  print(df.columns)
  # figure size in inches
  rcParams['figure.figsize'] = 11.7,8.27

  # Visualising the PM 2.5 Values
  sns.lineplot(data = df["PM2.5"], color="orange", label="PM2.5")
  plt.title('Visualising PM2.5 Data')
  plt.show()

def Correlation(df):
  df_corr = df.corr()
  plt.figure(figsize=(20,8))
  ax = sns.heatmap( df_corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), square=True, annot = True)
  ax.set_xticklabels(ax.get_xticklabels(),rotation=45, horizontalalignment='right')
  plt.show()

def Accept(df):
  pm_ = list(df['PM2.5'])
  Acceptable = []

  for pm in pm_:
    if pm >= 151:
        Acceptable.append(0)
    else:
        Acceptable.append(1)
        
  return Acceptable

def Normal(df):
  min_max_scale=MinMaxScaler()
  return min_max_scale.fit_transform(df.iloc[:,0:8].values)

#Below three functions are for calculating accuracy for ANN
def plot_confusion_matrix(cm, title='Confusion matrix', cmap="Blues"):
  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(4)
  plt.xticks(tick_marks, ['0', '1', '2', '3'])
  plt.yticks(tick_marks, ['0', '1', '2', '3'])
  plt.tight_layout()
  plt.ylabel('Expected ')
  plt.xlabel('Observed')
  plt.show()


def confuse(Expected, Observed):
  A_O = []
  A_E = []
  
  for i in Observed:
    if i >= 0.5 and i < 1:
      A_O.append(1)
    else:
      A_O.append(0)
    

  print("Precision : ", metrics.precision_score(Expected,A_O, average=None))
  print("Recall : ", metrics.recall_score(Expected,A_O, average=None))

  return metrics.confusion_matrix(Expected, A_O)


def acc_ANN(true, pred):
  sum = 0.0
  a = confuse(true, pred)
  print (a)
  # plot_confusion_matrix(a)
  for i in range(len(a)):
    for j in range(len(a)):
      if i == j:
        sum += a[i][j]

  print("Accuracy : ", (sum / len(true)) * 100)

warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=DeprecationWarning)
warnings.simplefilter(action='ignore', category=RuntimeWarning)

Train = pd.read_csv(r"/content/drive/MyDrive/DS-ML Project Assessment 4/Usable Data/Complete Data/Complete - Compile.csv")
Test = pd.read_csv(r"/content/drive/MyDrive/DS-ML Project Assessment 4/Usable Data/Complete Data/weather_2021.xlsx - Final.csv")

# Select duplicate rows except first occurrence based on all columns
duplicateRowsDF = Train[Train.duplicated()]
print("Duplicate Rows except first occurrence based on all columns are :")
print(duplicateRowsDF)

# Remove any duplicates
Train = Train[Train.duplicated() == False]
print(Train.shape)

#Initial Data Information
print("Viewing Intial Data Information")
Information(Train)

#Visualising Data
print("Visualising Data")
Visual(Train)
print("Correlation Matrix")
Correlation(Train)

"""
From the matrix we see ATM has very less correlation with the PM 2.5. So we can remove it to reduce calculation. (This won't affect the score of the model much)
"""

plt.figure(figsize=(15,10))
for i in Train.columns:
    plt.plot(Train[i])

plt.title('PM 2.5 Data Chart with ATM')
plt.ylabel('PM 2.5 Value')
plt.xlabel('Days')
plt.legend(['T', 'MaxT', 'MinT', 'ATM', 'RH', 'AvgVis', 'WS', 'MaxWS', 'PM2.5'], loc='upper left')
plt.show()

plt.figure(figsize=(15,10))
for i in Train.columns:
  if i == "ATM":
    continue
  plt.plot(Train[i])

plt.title('PM 2.5 Data Chart without ATM')
plt.ylabel('PM 2.5 Value')
plt.xlabel('Days')
plt.legend(['T', 'MaxT', 'MinT', 'RH', 'AvgVis', 'WS', 'MaxWS', 'PM2.5'], loc='upper left')
plt.show()

"""
1. Fine particulate matter (PM2.5) is an air pollutant that is a concern for people's health when levels in air are high. PM2.5 are tiny particles in the air that reduce visibility and cause the air to appear hazy when levels are elevated.
2. Outdoor PM2.5 levels are most likely to be elevated on days with little or no wind or air mixing.
"""

#Data Preprocessing
"""
We can use a binary feature Acceptable, it's value will be 1 if the PM 2.5 level is acceptable and 0 if not. Since it is calculated from PM 2.5 we can use this instead of PM 2.5.
"""

Train["Acceptable"] = Accept(Train)
print(Train.sample(3))

#figure size in inches
rcParams['figure.figsize'] = 4.75,4.75
print(Train.Acceptable.value_counts())
sns.countplot(y = "Acceptable", data = Train, palette = 'winter_r')
plt.show()

cols =  list(Train.columns)
plt.figure(figsize=(20, 20))
for i in range(1, 9):
  plt.subplot(3, 4, i)
  sns.scatterplot(x = cols[i - 1], y = Train['PM2.5'],data = Train, hue = "Acceptable", palette = "husl")

"""
Normalisation of data
"""
X = Normal(Train) #Training Input Feature
#print(X)
Y = (pd.DataFrame(Accept(Train), columns = ['Acceptable'])).values #Training Output
#print(Y.shape)

X_T = Normal(Test)
#print(X_T.shape)
y_T = pd.DataFrame(Accept(Test), columns = ['Acceptable'])
Y_T = y_T.values
#print(Y_T.shape)

result=[]

"""
Applying Linear Regression
"""
print("After Applying Linear Regression")
result.append(Regression(LinearRegression(),X,Y,X_T,Y_T))
#print(result)

"""
Applying Logistic Regression
"""
print("After Applying Logistic Regression")
result.append(Regression(LogisticRegression(solver="liblinear",random_state=47),X,Y,X_T,Y_T))
#print(result)

"""
Applying Support Vector Regression
"""
print("After Applying Support Vector Regression")
result.append(Regression(SVR(),X,Y,X_T,Y_T))
#print(result)

"""
Applying Decision Tree Regression
"""
print("After Applying Decision Tree Regression")
result.append(Regression(DecisionTreeRegressor(max_depth=1),X,Y,X_T,Y_T))
#print(result)

"""
Applying Random Forest Regression
"""
print("After Applying Random Forest Regression")
result.append(Regression(RandomForestRegressor(n_estimators=80),X,Y,X_T,Y_T))
#print(result)

"""
Applying Extra Tree Regressor"
"""
print("After Applying Extra tree Regression")
result.append(Regression(ExtraTreesRegressor(),X,Y,X_T,Y_T))
#print(result)

Result = pd.DataFrame(result,columns=["MAE","MSE","RMSE","R^2 Score"])
Result.insert(0, "Model", ["Linear Regression","Logistic Regression", "Support Vector Regression", "Decision Trees", "Random Forest","Extra Tree Regression"], True)
print(Result)

"""
Applying ANN
"""
classifier=Sequential()
classifier.add(Dense(7,activation = 'relu', input_dim = 8, kernel_regularizer=regularizers.l2(0.01)))
classifier.add(Dense(20,activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))
classifier.add(Dense(1,activation = 'sigmoid'))
classifier.compile(optimizer='adam',loss="binary_crossentropy",metrics=['accuracy'])
classifier.fit(X,Y,batch_size=10,epochs =100,verbose=None,shuffle=True)
p_ANN=classifier.predict(X_T)
#print("Accuracy: ",metrics.accuracy_score(Y_T,p_ANN))
#print(Analysis(classifier,X,Y,Y_T,p_ANN))
acc_ANN(Y_T,p_ANN)

Plots(Y,Y_T,p_ANN)

"""
Applying PCR
"""
pca = PCA()
X_pca = pca.fit_transform(X)
print(X_pca.shape)
X_t_pca = pca.transform(X_T)
print(pca.explained_variance_ratio_)
print(np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100))

PCR_result=[]
PCR_result.append(Regression(LinearRegression(),X_pca,Y,X_t_pca,Y_T))
PCR_result.append(Regression(LogisticRegression(solver="liblinear",random_state=47),X_pca,Y,X_t_pca,Y_T))
PCR_result.append(Regression(SVR(),X_pca,Y,X_t_pca,Y_T))
PCR_result.append(Regression(DecisionTreeRegressor(max_depth=1),X_pca,Y,X_t_pca,Y_T))
PCR_result.append(Regression(RandomForestRegressor(n_estimators=80),X_pca,Y,X_t_pca,Y_T))
#print(PCR_result)
PCR_Result=pd.DataFrame(PCR_result,columns=["MAE","MSE","RMSE","R^2 Score"])
PCR_Result.insert(0, "Model", ["Linear Regression", "Logistic Regression","Support Vector Regression", "Decision Trees", "Random Forest"], True)
print(PCR_Result)
#print(cross_val_score(lm, X_pca[:,:1], Y,scoring='r2'))

"""
Applying K-Means Clustering
"""
kmeans=KMeans(n_clusters=2,random_state=0).fit(X)
kpredictions = kmeans.predict(X_T)
print("Confusion Matrix\n",metrics.confusion_matrix(Y_T,kpredictions))
print("Accuracy: ",metrics.accuracy_score(Y_T,kpredictions))
print("Recall: ",metrics.recall_score(Y_T,kpredictions))
print("Precision: ",metrics.precision_score(Y_T,kpredictions))
print("F1 Score: ",metrics.f1_score(Y_T,kpredictions))
print(metrics.classification_report(Y_T,kpredictions))

plt.figure(figsize=(10,10))

plt.subplot(211)
plt.title('Confusion Matrix')
sns.heatmap(metrics.confusion_matrix(Y_T,kpredictions),annot=True) 

plt.subplot(212)
plt.title('Clusters')
plt.scatter(X_T[kpredictions==0,0],X_T[kpredictions==0,1],s=100,c='red')
plt.scatter(X_T[kpredictions==1,0],X_T[kpredictions==1,1],s=100,c='black')